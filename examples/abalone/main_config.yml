meta:
  data_name: "abalone"
  experiment_name: "trial_00"
  start_fresh: True
  # TODO: information on when to save params, currently only best params saved
logging:
  console:
    level: "info"
    format_str: null
  file:
    level: "ERROR"
    format_str: null
  track:
    tracker_steps: 30
    tensorboard:
      param_steps: 50
  # graph_spec: True

performance:
  objectives:
    # each objective can be a loss, a metric, or a combination of both
    main_obj:
      loss:
        # each loss can only be one loss
        # TODO: support custom losses
        type: "MSE"
        track: ["mean"] # only mean is supported currently
        #options:
      metric:
        # there can be many metrics
        # TODO: support custom metrics
        type: ["MeanSquaredError"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "reshape_out"
          target: "target_v"
        dataset: "abalone" # TODO: this could be infered if only one dataset is used?
    second_obj:
      loss:
        type: "MAE"
        track: ["mean"]
        #options:
      metric:
        type: ["meanabsoluteerror"]
        options: [null]
      in_config:
        type: "supervised"
        options:
          prediction: "reshape_out"
          target: "target_v"
        dataset: "abalone"

# TODO: this section needs to be redone
# TODO: this section should have different names for different datasets
data:
  datasets:
    "abalone":
      in:
        feature_a:
          shape: [2, 1]
          dtype: "float64"
        target_v:
          shape: [1, 1]
          dtype: "int32"
          label: True
      split:
        names: ["train", "val", "test"]
        # TODO: redo this section `train`, `val`, `eval`

optimize:
  # NOTE: multiple losses by the same optimizer, are currently only modeled
  # jointly, if we wish to model the losses seperately (sequentially or
  # alternating), then we would want to use a second optimizer
  # optimizers:
  #   "main_opt":
  #     type: 'adam'
  #     options:
  #       learning_rate: 0.0001
  #       beta_1: 0.91
  #     objectives: ["main_obj", "second_obj"]
  optimizers:
    "main_opt":
      type: "adam"
      options:
        learning_rate: 0.0001
        beta_1: 0.91
      objectives: ["main_obj"]
    "second_opt":
      type: "adam"
      options:
        learning_rate: 0.0002
        beta_1: 0.92
      objectives: ["second_obj"]
  directive:
    # (a|b): alternate, (a&b): joint, (a,b): sequential
    # eventually, we should be able to specify time/epoch/% before moving to the
    # next instruction, but presently each `opt` represents an (ENTIRE) pass of the training set
    # instruct: "(a|b)"
    instructions: "((main_opt, second_opt) & second_opt)"
    # TODO: handle case where not present
    #instructions: "(main_opt,second_opt)"

hyper_parameters:
  # optimizer_definition:
  #   seq: "(((a|b)&c),d)"
  # optimizer: # TODO: support multiple optimizers
  #   name: "main_2"
  #   type: 'adam'
  #   options:
  #     learning_rate: 0.001
  #     beta_1: 0.92
  epochs: 20
  dataset:
    # TODO: I would like to make this logic more abstract
    # I think the only options that should be applied here are "batch" and "shuffle"
    batch: 16
    shuffle_buffer: 128 # this should be grouped with batchsize
  # other:
  #   earlystopping:
  #     type: 'earlystopping'
  #     options:

model:
  path: "./model_config.yml"
