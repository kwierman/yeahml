# meta:
# name: "model_a"
# activation:
#   type: 'elu'

name: "model_a"
start_fresh: True

# TODO: it could be assumed that if only one layer in is defined in main, that
# it could be the `in_name` to the first layer.
layers:
  flatten_1:
    type: "flatten"
    in_name: "feature_a"
  dense_1:
    type: "dense"
    options:
      units: 16
    # in_name: 'feature_a'
  dense_2:
    type: "dense"
    options:
      units: 8
      kernel_constraint:
        type: MaxNorm
      bias_regularizer:
        type: "L1"
      activation:
        type: "relu"
  dense_2b:
    type: "dense"
    options:
      units: 8
      kernel_constraint:
        type: MaxNorm
        options:
          max_value: 3
      kernel_initializer:
        type: RandomNormal
      bias_regularizer:
        type: "L1"
        options:
          l1: 0.879
      activation:
        type: "relu"
        options:
          alpha: 0.9
    in_name: "dense_1"
  concat_1:
    type: "concatenate"
    in_name: ["dense_2", "dense_2b"]
  dense_3a:
    type: "dense"
    options:
      units: 8
      kernel_constraint:
        type: MaxNorm
      bias_regularizer:
        type: "L1"
      kernel_initializer:
        type: RandomNormal
        options:
          mean: 0.3
      activation:
        type: "relu"
  dense_3b:
    type: "dense"
    options:
      kernel_initializer:
        type: RandomNormal
      units: 8
      activation:
        type: "relu"
    in_name: "dense_2"
  concat_3:
    type: "concatenate"
    in_name: ["dense_3a", "dense_3b"]
  dense_out:
    type: "dense"
    options:
      units: 1
      activation:
        type: "linear"
  reshape_out:
    type: "reshape"
    options:
      target_shape: 1,1
    endpoint: True
